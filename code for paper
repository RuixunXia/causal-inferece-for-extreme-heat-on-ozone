#loading packages
library(readxl)
library(lme4)     
library(car)       
library(MatchIt)  
library(AER)       
library(dplyr)     
library(haven)
library(lmerTest) 
library(ggplot2)  
library(MASS)
library(sf)
library(ggspatial)
library(ggsci)
library(grid)
library(sjPlot)
library(car)
library(cobalt)
library(parallel)
library(logistf)
library(data.table)
library(future)
library(future.apply)
library(RTransferEntropy) 

### visualization and data preprocessing ###
# data preprocessing
# read data
df <- read_xlsx("finaldata.xlsx")
str(df)

# extract columns
new_df <- df %>%
  select(site_id, site, province, date, year, month, day, latitude,longtitude, observation_field_meter,
         airtemperature_mean, airtemperature_max, dewpointtemperature, sealevelpressure, winddirection, windspeedrate, 
         skyconditiontotalcoveragecode,v13,`_8h_max`)

# rename columns
new_df <- new_df %>%
  rename(longitude = longtitude,
         tem = airtemperature_mean,
         tem_max = airtemperature_max ,
         dew_tem = dewpointtemperature ,
         slp = sealevelpressure ,
         wd = winddirection ,
         wsr = windspeedrate ,
         sctc = skyconditiontotalcoveragecode ,
         lpd = v13 ,
         O3_8h_max = `_8h_max`)
colnames(new_df)

# check NA
# replace NA into mean
new_df$dew_tem <- ifelse(is.na(new_df$dew_tem), mean(new_df$dew_tem, na.rm = TRUE), new_df$dew_tem)
new_df$slp <- ifelse(is.na(new_df$slp), mean(new_df$slp, na.rm = TRUE), new_df$slp)
new_df$wd <- ifelse(is.na(new_df$wd), mean(new_df$wd, na.rm = TRUE), new_df$wd)
new_df$wsr <- ifelse(is.na(new_df$wsr), mean(new_df$wsr, na.rm = TRUE), new_df$wsr)
new_df$sctc <- ifelse(is.na(new_df$sctc), mean(new_df$sctc, na.rm = TRUE), new_df$sctc)
new_df$lpd <- ifelse(is.na(new_df$lpd), mean(new_df$lpd, na.rm = TRUE), new_df$lpd)
new_df$O3_8h_max <- ifelse(is.na(new_df$O3_8h_max), mean(new_df$O3_8h_max, na.rm = TRUE), new_df$O3_8h_max)

# unit transformation
new_df <- new_df %>%
  mutate(tem = tem/10,
         tem_max = tem_max/10,
         dew_tem = dew_tem/10,
         slp = slp/10,
         wsr = wsr/10,
         lpd = lpd/10)
new_df$latitude <- new_df$latitude / 100
new_df$longitude <- new_df$longitude / 100

# ozone transformation 
# box.cox transformation
p <- hist(new_df$O3_8h_max,
          main = "Distribution of MDA8 Ozone",
          xlab = "MDA8 Ozone",
          col = "skyblue",
          border = "white")

p <- ggplot(new_df, aes(x = O3_8h_max)) +
  geom_histogram(fill = "skyblue", color = "white", bins = 30) + 
  labs(title = "Distribution of MDA8 Ozone",
       x = "MDA8 Ozone",
       y = "Frequency") +
  theme_minimal()
print (p)
ggsave("D:/paper/ozone_distribution.png", plot = p, width = 10, height = 8, dpi = 600)

ozone_data <- new_df$O3_8h_max
if (any(ozone_data <= 0)) {
  ozone_data <- ozone_data + abs(min(ozone_data)) + 1
}

# best fitted lambda
boxcox_result <- boxcox(ozone_data ~ 1, plotit = TRUE)
best_lambda <- boxcox_result$x[which.max(boxcox_result$y)]

# apply box.cox transformation
if (best_lambda == 0) {
  # if lambda equals to 0 then using log transformation 
  new_df$O3_8h_max_boxcox <- log(ozone_data)
} else {
  new_df$O3_8h_max_boxcox <- (ozone_data^best_lambda - 1) / best_lambda
}

# histogram of box.cox ozone
hist(new_df$O3_8h_max_boxcox, breaks = 30, col = "skyblue", border = "black",
     main = "Histogram of Box-Cox Transformed Max 8-Hour Ozone Concentration",
     xlab = "Box-Cox Transformed Ozone Concentration (µg/m³)")

# establish binary variable
new_df$max_tem_35 <- ifelse(new_df$tem_max >= 35, 1, 0)

# variable standardization
new_df <- new_df %>%
  mutate(
    tem_max_scaled = scale(tem_max),
    tem_scaled = scale(tem),
    dew_tem_scaled = scale(dew_tem),
    slp_scaled = scale(slp),
    wd_scaled = scale(wd),
    wsr_scaled = scale(wsr),
    sctc_scaled = scale(sctc),
    lpd_scaled = scale(lpd)
  )

save(new_df, file = "D:/研究数据/new_df.RData") # after data preprocessing


# sites distribution map
setwd("D:/Paper/202405更新_2024年省市县三级行政区划数据（审图号：GS（2024）0650号）/shp格式的数据（调整过行政区划代码，补全省市县信息）")

china_province <- read_sf("省.shp")
ten_dashed_line <- read_sf("十段线.shp")
ten_dashed_line <- st_transform(ten_dashed_line, crs = st_crs(china_province))

p <- ggplot() +
  geom_sf(data = china_province, fill = "lightgrey", color = "black", linewidth = 0.5) +
  geom_sf(data = ten_dashed_line, color = "black", linewidth = 1.2, linetype = "dashed") +
  geom_point(data = new_df, aes(x = longitude, y = latitude),
             color = "darkblue", size = 1.5, alpha = 0.8) +
  annotation_scale(location = "bl", width_hint = 0.3, text_cex = 0.8, line_width = 1) +
  annotation_north_arrow(location = "tr", which_north = "true",
                         style = north_arrow_fancy_orienteering,
                         height = unit(1, "cm"), width = unit(1, "cm")) +
  labs(title = "Site Distribution", x = "Longitude", y = "Latitude") +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "lightgray", linewidth = 0.1),
        panel.grid.minor = element_blank(),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12, face = "bold"),
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
print(p)
ggsave("D:/paper/site_distribution.png", plot = p, width = 10, height = 8, dpi = 600) # save figure


### Statistical analysis part ####
# National Level #

# linear mixed effects model LMM
lmm_model <- lmer(O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + wd_scaled +
                    wsr_scaled + sctc_scaled + lpd_scaled + (1 | site_id),
                  data = new_df)
summary(lmm_model)
ranova(lmm_model)

vif_values <- vif(lmm_model)
print(vif_values)

# Create a Word table for the model output with p-values using tab_model()
tab_model(lmm_model, 
          file = "D:/研究数据/lmm_output.doc",   # Saves the table as a Word document
          show.p = TRUE,             # Show p-values in the output
          show.se = TRUE,            # Show standard errors in the output
          digits = 2)              # Set the number of decimal places

# 14.2.Instrumental variable method IV
# altitude as IV TSLS
iv_model_observation <- ivreg(O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled |
                                observation_field_meter + dew_tem_scaled + slp_scaled + wd_scaled +
                                wsr_scaled + sctc_scaled + lpd_scaled, data = new_df)
summary(iv_model_observation, diagnostics = TRUE) 

# Create a three-line formatted table 
tab_model(iv_model_observation, 
          file = "D:/研究数据/TSLS_Model_Summary.doc",   # Saves the table as a Word document
          show.p = TRUE,             # Show p-values in the output
          show.se = TRUE,            # Show standard errors in the output
          digits = 2)                # Set the number of decimal places

#Heckman two-step selection model
probit_model <- glm(max_tem_35 ~ observation_field_meter + dew_tem_scaled + slp_scaled + wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled, family = binomial(link = "probit"), data = new_df)
summary(probit_model)

new_df$predicted_max_tem_35 <- predict(probit_model, type = "link")
new_df$IMR <- dnorm(new_df$predicted_max_tem_35) / pnorm(new_df$predicted_max_tem_35)

lmm_model_iv_heckman <- lmer(O3_8h_max_boxcox ~ max_tem_35 + IMR + dew_tem_scaled + slp_scaled + wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled + (1 | site_id),
                             data = new_df) 
summary(lmm_model_iv_heckman)
tab_model(lmm_model_iv_heckman, 
          file = "D:/研究数据/lmm_model_iv_heckman_output.doc",   # Saves the table as a Word document
          show.p = TRUE,             # Show p-values in the output
          show.se = TRUE,            # Show standard errors in the output
          digits = 2)                # Set the number of decimal places

# Propensity score analysis
# extract propensity score
logit_model <- glm(max_tem_35 ~ latitude + observation_field_meter + month + tem_scaled + dew_tem_scaled + slp_scaled + wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled, family = binomial,
                   data = new_df) 
summary(logit_model)
new_df$propensity_score <- predict(logit_model, type = "response")

# propensity score calibration PSC
lmm_model_psc <- lmer(O3_8h_max_boxcox ~ max_tem_35 + propensity_score + (1 | site_id), data = new_df)
summary(lmm_model_psc)
ranova(lmm_model_psc)

# Create a Word table for the model output with p-values using tab_model()
tab_model(lmm_model_psc, 
          file = "D:/研究数据/lmm_model_psc_output.doc",   # Saves the table as a Word document
          show.p = TRUE,             # Show p-values in the output
          show.se = TRUE,            # Show standard errors in the output
          digits = 2)              # Set the number of decimal places


# propensity score matching PSM
# many to one NN matching with replacement
psm_match_1 <- matchit(as.factor(max_tem_35) ~ month + latitude + observation_field_meter + tem + dew_tem + slp + wsr + sctc, 
                       data = new_df, 
                       method = "nearest",   # Nearest neighbor matching
                       distance = "logit",   # Propensity score model based on logistic regression
                       replace = TRUE,       # Allow replacement so that controls can be reused
                       ratio = 2)            # Many-to-one matching (e.g., 2 control units per treated unit)
summary(psm_match_1)
plot(summary(psm_match_1)) #love plot
love.plot(psm_match_1, stats = c("c","ks"),
          thresholds = c(cor = .1),
          abs = TRUE, warp = 20,
          limits = list(ks = c(0,.5)),
          var.order = "unadjusted", line = TRUE)
plot(psm_match_1, type = 'hist') #histogram

# extract matched data
matched_data_1 <- match.data(psm_match_1)
save(matched_data_1, file = "D:/研究数据/matched_data_1.RData")

# estimate the treatment effect (ATT)
# Separate treated and control groups in the matched dataset
treated <- matched_data_1[matched_data_1$max_tem_35 == 1, ]
control <- matched_data_1[matched_data_1$max_tem_35 == 0, ]

# Calculate weighted means for treated and control groups
mean_treated <- weighted.mean(treated$O3_8h_max_boxcox, treated$weights, na.rm = TRUE)
mean_control <- weighted.mean(control$O3_8h_max_boxcox, control$weights, na.rm = TRUE)
ATT <- mean_treated - mean_control
print(paste("Estimated ATT of high temperature on ozone levels:", ATT))

# or using linear mixed effects regression model
att_model <- lmer(O3_8h_max_boxcox ~ max_tem_35 + (1|site_id), data = matched_data_1, weights = weights)
summary(att_model)
ranova(att_model)
# Create a Word table for the model output with p-values using tab_model()
tab_model(att_model, 
          file = "D:/研究数据/att_model_lmm_output.doc",   # Saves the table as a Word document
          show.p = TRUE,             # Show p-values in the output
          show.se = TRUE,            # Show standard errors in the output
          digits = 2)              # Set the number of decimal places

# inverse probability treatment weighting IPTW
new_df$weight <- ifelse(new_df$max_tem_35 == 1,
                        1 / new_df$propensity_score,                # treated
                        1 / (1 - new_df$propensity_score))          # controlled

# Calculate stabilized weights
p_treat <- mean(new_df$max_tem_35) # Proportion of treated units
p_control <- 1 - p_treat # Proportion of control units

stabilized_weights <- ifelse(new_df$max_tem_35 == 1, p_treat/new_df$propensity_score, 
                             p_control / (1 - new_df$propensity_score))

# Normalize the weights so that their sum equals the sample size
normalized_weights <- stabilized_weights / mean(stabilized_weights)

# Adding normalized weights to the data frame
new_df$normalized_weights <- normalized_weights

weighted_model_lmm <- lmer(O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + 
                             wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled + (1 | site_id), data = new_df, weights = normalized_weights)
summary(weighted_model_lmm)
# Create a Word table for the model output with p-values using tab_model()
tab_model(weighted_model_lmm, 
          file = "D:/研究数据/weighted_model_lmm_output.doc",   # Saves the table as a Word document
          show.p = TRUE,             # Show p-values in the output
          show.se = TRUE,            # Show standard errors in the output
          digits = 2)              # Set the number of decimal places


# Generalized bootstrap GB
# Increase propensity score cutoff to prevent extreme values
trim_propensity <- function(ps, trim_level = 0.01) {
  ps <- pmax(ps, trim_level)
  ps <- pmin(ps, 1 - trim_level)
  ps
}

fit_weighted_lmm <- function(data, indices, treated_idx, control_idx) {
  # Extract data based on the index of the bootstrap sample
  bootstrap_data <- data[indices, ]
  
  # Fitting the model using weighted least squares
  fit <- try({
    lmer(
      O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + 
        wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled + (1 | site_id),
      data = bootstrap_data,
      weights = bootstrap_data$weight,  
      control = lmerControl(
        optimizer = "nloptwrap",
        calc.derivs = FALSE,  
        optCtrl = list(maxfun = 500)
      )
    )
  }, silent = TRUE)
  
  if (inherits(fit, "try-error")) {
    return(NA)
  }
  return(fixef(fit)["max_tem_35"])
}

run_future_bootstrap <- function(n_bootstrap, data, strata, weights) {
  # Precompute the indices for the treatment and control groups
  treated_idx <- which(data$max_tem_35 == 1)
  control_idx <- which(data$max_tem_35 == 0)
  
  # Optimizing data structures using data.table
  data <- as.data.table(data)
  
  # Setting up a parallel environment
  plan(multisession, workers = max(detectCores() - 2, 1)) 
  
  # Parallel Bootstrap
  bootstrap_results <- future_lapply(1:n_bootstrap, function(i) {
    # Random sampling index (with strata and weights)
    indices <- sample(1:nrow(data), size = nrow(data), replace = TRUE, prob = weights)
    # Fitting a function using a weighted model
    fit_weighted_lmm(data, indices, treated_idx, control_idx)
  })
  
  # extract effective results
  estimates <- unlist(bootstrap_results)
  convergence_rate <- mean(!is.na(estimates))
  
  list(
    estimates = estimates[!is.na(estimates)],  
    convergence_rate = convergence_rate
  )
}

set.seed(123)

# calculating propensity score with trim
model_ps_firth <- logistf(
  max_tem_35 ~ latitude + observation_field_meter + month + tem_scaled + 
    dew_tem_scaled + slp_scaled + wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled,
  data = new_df
)
new_df$propensity_score <- trim_propensity(predict(model_ps_firth, type = "response"))

# defining strata and weights
strata <- new_df$max_tem_35
weights <- ifelse(
  new_df$max_tem_35 == 1,
  (1 / new_df$propensity_score) / sum(1 / new_df$propensity_score),
  (1 / (1 - new_df$propensity_score)) / sum(1 / (1 - new_df$propensity_score))
)

# running bootstrap using 1000 iterative
bootstrap_results <- run_future_bootstrap(1000, new_df, strata, weights)

# results
valid_estimates <- bootstrap_results$estimates
cat("优化后的Bootstrap结果：\n",
    "有效迭代次数:", length(valid_estimates), "\n",
    "模型收敛率:", bootstrap_results$convergence_rate, "\n",
    "平均效应:", mean(valid_estimates), "\n",
    "标准误:", sd(valid_estimates), "\n",
    "95%置信区间:", quantile(valid_estimates, c(0.025, 0.975)))

# Transfer Entropy TE
target_provinces <- c("辽宁", "浙江", "河北", "新疆", "北京", "广西", "江苏", "江西", 
                      "福建", "安徽", "内蒙古", "山西", "天津", "河南", "四川", "湖北", 
                      "湖南", "广东", "山东", "云南", "海南", "青海", "贵州", "宁夏", 
                      "吉林", "黑龙江", "陕西", "甘肃", "西藏")
tryCatch({
  cat("\n=== 正在处理全国数据 ===\n")
  

  national_data <- new_df %>%
    filter(province %in% target_provinces) %>%
    select(predicted_max_tem_35, O3_8h_max_boxcox) %>%
    na.omit()
  

  national_temp <- national_data$predicted_max_tem_35
  national_ozone <- national_data$O3_8h_max_boxcox
  

  national_te_result <- transfer_entropy(national_temp, national_ozone, lx = 2, ly = 1, nboot = 100)
  

  national_te_temp_to_ozone <- national_te_result$coef[1, "te"]
  national_te_ozone_to_temp <- national_te_result$coef[2, "te"]
  national_p_temp_to_ozone <- national_te_result$coef[1, "p-value"]
  national_p_ozone_to_temp <- national_te_result$coef[2, "p-value"]
  

  cat("\n=== 全国数据结果 ===\n")
  cat("从温度到臭氧的转移熵:", national_te_temp_to_ozone, "p值:", national_p_temp_to_ozone, "\n")
  cat("从臭氧到温度的转移熵:", national_te_ozone_to_temp, "p值:", national_p_ozone_to_temp, "\n")

  
  if (national_te_temp_to_ozone > national_te_ozone_to_temp) {
    print("全国范围内温度先于臭氧")
  } else if (national_te_temp_to_ozone < national_te_ozone_to_temp) {
    print("全国范围内臭氧先于温度")
  } else {
    print("全国范围内温度和臭氧影响相当")
  }
  

  signif_stars <- c("***" = 0.001, "**" = 0.01, "*" = 0.05, "." = 0.1)
  signif_temp_to_ozone <- names(signif_stars)[which.min(abs(signif_stars - national_p_temp_to_ozone))]
  signif_ozone_to_temp <- names(signif_stars)[which.min(abs(signif_stars - national_p_ozone_to_temp))]
  
  cat("从温度到臭氧的显著性:", ifelse(national_p_temp_to_ozone < 0.1, signif_temp_to_ozone, "不显著"), "\n")
  cat("从臭氧到温度的显著性:", ifelse(national_p_ozone_to_temp < 0.1, signif_ozone_to_temp, "不显著"), "\n")
  

  national_result <- list(
    te_temp_to_ozone = national_te_temp_to_ozone,
    te_ozone_to_temp = national_te_ozone_to_temp,
    p_temp_to_ozone = national_p_temp_to_ozone,
    p_ozone_to_temp = national_p_ozone_to_temp,
    signif_temp_to_ozone = ifelse(national_p_temp_to_ozone < 0.1, signif_temp_to_ozone, "不显著"),
    signif_ozone_to_temp = ifelse(national_p_ozone_to_temp < 0.1, signif_ozone_to_temp, "不显著")
  )
  
}, error = function(e) {
  cat("\n!!! 处理全国数据时出错: ", e$message, "\n")
})



# Province Level #
# loading package
library(lme4) 
library(lmerTest)
library(sjPlot)
library(dplyr)
library(purrr)
library(lme4)
library(lmerTest)
library(broom)
library(broom.mixed)
library(flextable)
library(officer)
library(writexl)
library(parallel)
library(logistf)
library(data.table)
library(future)
library(future.apply)
library(RTransferEntropy) 

# LMM by province
lmm <- lmer(O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled + (1 | province),
            data = new_df)
summary(lmm)
ranova(lmm) #statistically significant for province

model_results <- new_df %>%
  group_by(province) %>%
  group_split() %>%
  map_dfr(function(df_grp) {
    prov <- unique(df_grp$province)
    n_sites <- n_distinct(df_grp$site_id)
    
    if (n_distinct(df_grp$max_tem_35) < 2) {
      return(tibble(
        province = prov,
        effect_max_tem_35 = NA,
        se_max_tem_35 = NA,
        p_max_tem_35 = NA,
        model_type = "Skipped"
      ))
    }
    
    if (n_sites == 1) {
      model <- tryCatch({
        lm(O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + wd_scaled +
             wsr_scaled + sctc_scaled + lpd_scaled, data = df_grp)
      }, error = function(e) return(NULL))
      
      if (is.null(model)) {
        return(tibble(
          province = prov,
          effect_max_tem_35 = NA,
          se_max_tem_35 = NA,
          p_max_tem_35 = NA,
          model_type = "LM-Failed"
        ))
      }
      
      coef_df <- tidy(model)
      max_tem_row <- coef_df %>% filter(term == "max_tem_35")
      
      if (nrow(max_tem_row) == 0) {
        return(tibble(
          province = prov,
          effect_max_tem_35 = NA,
          se_max_tem_35 = NA,
          p_max_tem_35 = NA,
          model_type = "LM-No max_tem_35"
        ))
      }
      
      return(tibble(
        province = prov,
        effect_max_tem_35 = max_tem_row$estimate,
        se_max_tem_35 = max_tem_row$std.error,
        p_max_tem_35 = max_tem_row$p.value,
        model_type = "LM"
      ))
    }
    
    model <- tryCatch({
      lmer(O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + wd_scaled +
             wsr_scaled + sctc_scaled + lpd_scaled + (1 | site_id), data = df_grp)
    }, error = function(e) return(NULL))
    
    if (is.null(model)) {
      return(tibble(
        province = prov,
        effect_max_tem_35 = NA,
        se_max_tem_35 = NA,
        p_max_tem_35 = NA,
        model_type = "LMM-Failed"
      ))
    }
    
    coef_df <- tidy(model)
    max_tem_row <- coef_df %>% filter(term == "max_tem_35")
    
    if (nrow(max_tem_row) == 0) {
      return(tibble(
        province = prov,
        effect_max_tem_35 = NA,
        se_max_tem_35 = NA,
        p_max_tem_35 = NA,
        model_type = "LMM-No max_tem_35"
      ))
    }
    
    return(tibble(
      province = prov,
      effect_max_tem_35 = max_tem_row$estimate,
      se_max_tem_35 = max_tem_row$std.error,
      p_max_tem_35 = max_tem_row$p.value,
      model_type = "LMM"
    ))
  }) %>%

  mutate(
    significance = case_when(
      is.na(p_max_tem_35) ~ NA_character_,
      p_max_tem_35 < 0.001 ~ "***",
      p_max_tem_35 < 0.01 ~ "**",
      p_max_tem_35 < 0.05 ~ "*",
      TRUE ~ ""
    ),
    p_star = case_when(
      is.na(p_max_tem_35) ~ NA_character_,
      p_max_tem_35 < 0.001 ~ paste0(format(p_max_tem_35, scientific = FALSE), "***"),
      p_max_tem_35 < 0.01 ~ paste0(format(p_max_tem_35, scientific = FALSE), "**"),
      p_max_tem_35 < 0.05 ~ paste0(format(p_max_tem_35, scientific = FALSE), "*"),
      TRUE ~ format(p_max_tem_35, scientific = FALSE)
    )
  )


model_results <- model_results %>%
  mutate(
    effect_max_tem_35 = round(effect_max_tem_35, 3),
    se_max_tem_35 = round(se_max_tem_35, 3),
    p_max_tem_35 = round(p_max_tem_35, 3),
    p_star = case_when(
      is.na(p_max_tem_35) ~ NA_character_,
      p_max_tem_35 < 0.001 ~ paste0(format(round(p_max_tem_35, 3), nsmall = 3), "***"),
      p_max_tem_35 < 0.01  ~ paste0(format(round(p_max_tem_35, 3), nsmall = 3), "**"),
      p_max_tem_35 < 0.05  ~ paste0(format(round(p_max_tem_35, 3), nsmall = 3), "*"),
      TRUE                 ~ format(round(p_max_tem_35, 3), nsmall = 3)
    )
  )

write_xlsx(model_results, "D:/paper/lmm_or_lm_by_province.xlsx")

# 2. IPTW by province
iptw_results_by_province <- new_df %>%
  group_by(province) %>%
  group_split() %>%
  map_dfr(function(df_grp) {
    prov <- unique(df_grp$province)
    
    if (length(unique(df_grp$max_tem_35)) < 2) {
      return(data.frame(province = prov, status = "Skipped: only one treatment level"))
    }
    
    # Step 1: Logistic regression
    logit_model <- tryCatch({
      glm(max_tem_35 ~ latitude + observation_field_meter + month +
            tem_scaled + dew_tem_scaled + slp_scaled +
            wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled,
          family = binomial(link = "logit"),
          data = df_grp)
    }, error = function(e) return(NULL))
    
    if (is.null(logit_model)) {
      return(data.frame(province = prov, status = "Skipped: logit model failed"))
    }
    
    # Step 2: propensity score
    df_grp$propensity_score <- predict(logit_model, type = "response")
    
    # Step 3: weighting
    p_treat <- mean(df_grp$max_tem_35)
    p_control <- 1 - p_treat
    stabilized_weights <- ifelse(df_grp$max_tem_35 == 1,
                                 p_treat / df_grp$propensity_score,
                                 p_control / (1 - df_grp$propensity_score))
    normalized_weights <- stabilized_weights / mean(stabilized_weights)
    df_grp$normalized_weights <- normalized_weights
    
    # Step 4: number of sites
    n_sites <- length(unique(df_grp$site_id))
    
    if (n_sites >= 2) {
      # using lmm
      model <- tryCatch({
        lmer(O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + 
               wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled + (1 | site_id),
             data = df_grp, weights = normalized_weights)
      }, error = function(e) return(NULL))
      
      if (is.null(model)) {
        return(data.frame(province = prov, status = "Skipped: LMM model failed"))
      }
      
      coef_summary <- summary(model)$coefficients
      ranova_out <- suppressMessages(ranova(model))
      site_var <- tryCatch({
        round(attr(ranova_out, "varcor")$site_id[1], 3)
      }, error = function(e) NA)
      
      return(data.frame(
        province = prov,
        effect_max_tem_35 = round(coef_summary["max_tem_35", "Estimate"], 3),
        se_max_tem_35 = round(coef_summary["max_tem_35", "Std. Error"], 3),
        p_max_tem_35 = round(coef_summary["max_tem_35", "Pr(>|t|)"], 3),
        ranova_site_var = site_var,
        model_type = "LMM",
        status = "OK"
      ))
      
    } else {
      # using lm
      model <- tryCatch({
        lm(O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + 
             wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled,
           data = df_grp, weights = normalized_weights)
      }, error = function(e) return(NULL))
      
      if (is.null(model)) {
        return(data.frame(province = prov, status = "Skipped: LM model failed"))
      }
      
      coef_summary <- summary(model)$coefficients
      
      return(data.frame(
        province = prov,
        effect_max_tem_35 = round(coef_summary["max_tem_35", "Estimate"], 3),
        se_max_tem_35 = round(coef_summary["max_tem_35", "Std. Error"], 3),
        p_max_tem_35 = round(coef_summary["max_tem_35", "Pr(>|t|)"], 3),
        ranova_site_var = NA,
        model_type = "LM",
        status = "OK"
      ))
    }
  })

print(iptw_results_by_province)

iptw_table <- iptw_results_by_province %>%
  mutate(
    significance = case_when(
      p_max_tem_35 < 0.001 ~ "***",
      p_max_tem_35 < 0.01 ~ "**",
      p_max_tem_35 < 0.05 ~ "*",
      TRUE ~ ""
    ),
    p_star = paste0(p_max_tem_35, significance)
  )

write_xlsx(iptw_table, "D:/paper/iptw_results_by_province.xlsx")

# GB by province
trim_propensity <- function(ps, trim_level = 0.01) {
  ps <- pmax(ps, trim_level)  
  ps <- pmin(ps, 1 - trim_level) 
  ps
}


fit_weighted_lm <- function(data, indices) {
  bootstrap_data <- data[indices, ]
  fit <- try({
    lm(
      O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + 
        wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled,
      data = bootstrap_data,
      weights = bootstrap_data$weight
    )
  }, silent = TRUE)
  if (inherits(fit, "try-error")) return(NA)
  return(coef(fit)["max_tem_35"])
}


fit_weighted_lmm <- function(data, indices, treated_idx, control_idx) {
  bootstrap_data <- data[indices, ]
  fit <- try({
    lmer(
      O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + 
        wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled + (1 | site_id),
      data = bootstrap_data,
      weights = bootstrap_data$weight,
      control = lmerControl(
        optimizer = "nloptwrap",
        calc.derivs = FALSE,
        optCtrl = list(maxfun = 1000)
      )
    )
  }, silent = TRUE)
  if (inherits(fit, "try-error")) return(NA)
  return(fixef(fit)["max_tem_35"])
}


run_province_bootstrap <- function(n_bootstrap, data, use_lm = FALSE) {
  data <- as.data.table(data)
  plan(multisession, workers = max(detectCores() - 2, 1))
  
  if (use_lm) {
  
    bootstrap_results <- future_lapply(1:n_bootstrap, function(i) {
      indices <- sample(1:nrow(data), size = nrow(data), replace = TRUE, prob = data$weight)
      fit_weighted_lm(data, indices)
    })
  } else {

    treated_idx <- which(data$max_tem_35 == 1)
    control_idx <- which(data$max_tem_35 == 0)
    bootstrap_results <- future_lapply(1:n_bootstrap, function(i) {
      indices <- sample(1:nrow(data), size = nrow(data), replace = TRUE, prob = data$weight)
      fit_weighted_lmm(data, indices, treated_idx, control_idx)
    })
  }
  
  estimates <- unlist(bootstrap_results)
  convergence_rate <- mean(!is.na(estimates))
  list(
    estimates = estimates[!is.na(estimates)],
    convergence_rate = convergence_rate
  )
}


analyze_by_province <- function(target_provinces, full_data, n_bootstrap = 1000) {
  all_results <- list()
  
  for (target_province in target_provinces) {
    tryCatch({
      cat("\n=== 正在处理省份:", target_province, "===\n")
      
      province_data <- full_data %>%
        filter(grepl(paste0("^", target_province), province)) %>%
        na.omit()
      
      if (nrow(province_data) == 0) {
        cat("  错误：该省份无有效数据\n")
        next
      }
      
      treated_count <- sum(province_data$max_tem_35 == 1)
      control_count <- sum(province_data$max_tem_35 == 0)
      if (treated_count == 0 || control_count == 0) {
        cat("  错误：该省份缺少处理组或对照组数据\n")
        next
      }
      cat("  样本量：总观测=", nrow(province_data), 
          "，高温组=", treated_count, 
          "，非高温组=", control_count, "\n", sep = "")
      
   
      cat("  计算倾向得分...\n")
      if (target_province %in% c("北京", "四川", "天津")) {
        model_ps <- logistf(
          max_tem_35 ~ month + tem_scaled + dew_tem_scaled + slp_scaled + 
            wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled,
          data = province_data
        )
      } else {
        model_ps <- logistf(
          max_tem_35 ~ latitude + observation_field_meter + month + tem_scaled + 
            dew_tem_scaled + slp_scaled + wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled,
          data = province_data
        )
      }
      
      province_data$propensity_score <- trim_propensity(predict(model_ps, type = "response"))
      province_data <- province_data %>%
        mutate(
          weight = ifelse(
            max_tem_35 == 1,
            (1 / propensity_score) / sum(1 / propensity_score),
            (1 / (1 - propensity_score)) / sum(1 / (1 - propensity_score))
          )
        )
      

      cat("  运行", n_bootstrap, "次Bootstrap...\n", sep = "")
      use_lm <- (target_province %in% c("北京", "四川", "天津"))
      bootstrap_res <- run_province_bootstrap(n_bootstrap, province_data, use_lm)
      
      valid_est <- bootstrap_res$estimates
      if (length(valid_est) == 0) {
        cat("  警告：所有Bootstrap迭代均失败\n")
        

        final_model <- if (use_lm) {
          try(lm(
            O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + 
              wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled,
            data = province_data,
            weights = province_data$weight
          ), silent = TRUE)
        } else {
          try(lmer(
            O3_8h_max_boxcox ~ max_tem_35 + dew_tem_scaled + slp_scaled + 
              wd_scaled + wsr_scaled + sctc_scaled + lpd_scaled + (1 | site_id),
            data = province_data,
            weights = province_data$weight,
            control = lmerControl(optimizer = "nloptwrap", optCtrl = list(maxfun = 2000))
          ), silent = TRUE)
        }
        
        if (inherits(final_model, "try-error")) {
          cat("  最终模型拟合失败\n")
          next
        }
        
        effect_coef <- if (use_lm) coef(final_model)["max_tem_35"] else fixef(final_model)["max_tem_35"]
        cat("  使用最终模型结果：平均效应=", round(effect_coef, 4), "\n", sep = "")
        
        all_results[[target_province]] <- list(
          province = target_province,
          n_obs = nrow(province_data),
          n_treated = treated_count,
          n_control = control_count,
          convergence_rate = 0,
          effect_mean = effect_coef,
          effect_se = NA,
          ci_lower = NA,
          ci_upper = NA,
          p_value = NA  
        )
        next
      }
      

      mean_effect <- mean(valid_est)
      if (mean_effect > 0) {
        p_value <- 2 * mean(valid_est <= 0)
      } else {
        p_value <- 2 * mean(valid_est >= 0)
      }
      p_value <- min(p_value, 1)
      

      all_results[[target_province]] <- list(
        province = target_province,
        n_obs = nrow(province_data),
        n_treated = treated_count,
        n_control = control_count,
        convergence_rate = bootstrap_res$convergence_rate,
        effect_mean = mean_effect,
        effect_se = sd(valid_est),
        ci_lower = quantile(valid_est, 0.025),
        ci_upper = quantile(valid_est, 0.975),
        p_value = p_value
      )
      
      cat("  分析完成：平均效应=", round(mean_effect, 4), 
          "，p值=", round(p_value, 4),
          "，收敛率=", round(bootstrap_res$convergence_rate, 2), "\n", sep = "")
      
    }, error = function(e) {
      cat("  处理失败：", conditionMessage(e), "\n", sep = "")
    })
  }
  
  return(all_results)
}


set.seed(123)
target_provinces <- c("辽宁", "浙江", "河北", "北京", "广西", "江苏", "江西", 
                      "福建", "安徽", "山西", "天津", "河南", "四川", "湖北", 
                      "广东", "山东", "黑龙江")

province_results <- analyze_by_province(
  target_provinces = target_provinces,
  full_data = new_df,
  n_bootstrap = 1000  
)


if (length(province_results) > 0) {
  result_table <- data.frame(
    省份 = sapply(province_results, function(x) x$province),
    总样本量 = sapply(province_results, function(x) x$n_obs),
    高温组样本 = sapply(province_results, function(x) x$n_treated),
    非高温组样本 = sapply(province_results, function(x) x$n_control),
    模型类型 = sapply(province_results, function(x) if (x$province %in% c("北京", "四川", "天津")) "普通线性" else "混合效应"),
    模型收敛率 = sapply(province_results, function(x) round(x$convergence_rate, 3)),
    平均效应值 = sapply(province_results, function(x) round(x$effect_mean, 4)),
    效应标准误 = sapply(province_results, function(x) round(x$effect_se, 4)),
    置信区间下限 = sapply(province_results, function(x) round(x$ci_lower, 4)),
    置信区间上限 = sapply(province_results, function(x) round(x$ci_upper, 4)),
    p值 = sapply(province_results, function(x) round(x$p_value, 4)),
    check.names = FALSE
  )
  
  cat("\n=== 各省份分析结果汇总 ===\n")
  print(result_table, row.names = FALSE)
  
  write.csv(result_table, "/Users/swifty/Desktop/各省份高温对臭氧影响分析结果.csv", 
            row.names = FALSE, fileEncoding = "UTF-8")
  cat("\n结果已保存至：各省份高温对臭氧影响分析结果.csv\n")
} else {
  cat("\n所有省份分析均失败，请检查数据或模型设置。\n")
}



# TE
te_results <- list()
signif_stars <- c("***" = 0.001, "**" = 0.01, "*" = 0.05, "." = 0.1) 

# parameter setting
target_provinces <- c("辽宁", "浙江", "河北", "北京", "广西", "江苏", "江西", "安徽",
                      "福建", "山西", "天津", "河南", "四川", "湖北", 
                      "广东", "山东", "吉林", "黑龙江")

for (target_province in target_provinces) {
  tryCatch({
    cat("\n=== 正在处理省份:", target_province, "===\n")
    

    province_sites <- new_df %>%
      filter(grepl(paste0("^", target_province), province)) %>%
      select(predicted_max_tem_35, O3_8h_max_boxcox) %>%
      na.omit()  
    
    temp <- province_sites$predicted_max_tem_35
    ozone <- province_sites$O3_8h_max_boxcox
    
    te_result <- transfer_entropy(temp, ozone, lx = 2, ly = 1, nboot = 100)
    
    te_results[[target_province]] <- te_result
    cat("\n转移熵计算完成:", target_province, "\n")
    
  }, error = function(e) {
    cat("\n!!! 处理省份", target_province, "时出错: ", e$message, "\n")
  })
}

# TE by province
for (target_province in target_provinces) {
  te_result <- te_results[[target_province]]
  
  # Extract the transfer entropy coefficients from temperature to ozone and from ozone to extreme heat
  te_temp_to_ozone <- te_result$coef[1, "te"]
  te_ozone_to_temp <- te_result$coef[2, "te"]
  
  p_temp_to_ozone <- te_result$coef[1, "p-value"]
  p_ozone_to_temp <- te_result$coef[2, "p-value"]
  
  # results
  cat("\n=== 正在处理省份:", target_province, "===\n")
  cat("从温度到臭氧的转移熵:", te_temp_to_ozone, "p值:", p_temp_to_ozone, "\n")
  cat("从臭氧到温度的转移熵:", te_ozone_to_temp, "p值:", p_ozone_to_temp, "\n")
  
  # Determine the size of transfer entropy
  if (te_temp_to_ozone > te_ozone_to_temp) {
    print(paste(target_province, "温度先于臭氧"))
  } else if (te_temp_to_ozone < te_ozone_to_temp) {
    print(paste(target_province, "臭氧先于温度"))
  } else {
    print(paste(target_province, "温度和臭氧影响相当"))
  }
  
  # Significance
  if (p_temp_to_ozone < 0.001) {
    print("从温度到臭氧的影响显著 (p < 0.001)")
  } else if (p_temp_to_ozone < 0.01) {
    print("从温度到臭氧的影响显著 (p < 0.01)")
  } else if (p_temp_to_ozone < 0.05) {
    print("从温度到臭氧的影响显著 (p < 0.05)")
  } else {
    print("从温度到臭氧的影响不显著")
  }
  
  if (p_ozone_to_temp < 0.001) {
    print("从臭氧到温度的影响显著 (p < 0.001)")
  } else if (p_ozone_to_temp < 0.01) {
    print("从臭氧到温度的影响显著 (p < 0.01)")
  } else if (p_ozone_to_temp < 0.05) {
    print("从臭氧到温度的影响显著 (p < 0.05)")
  } else {
    print("从臭氧到温度的影响不显著")
  }
}


# Create an empty data frame to store the results
result_df <- data.frame(
  Province = character(),
  TE_Temp_to_Ozone = numeric(),
  P_Temp_to_Ozone = character(),
  TE_Ozone_to_Temp = numeric(),
  P_Ozone_to_Temp = character(),
  Dominant_Direction = character(),
  stringsAsFactors = FALSE
)


result_df <- rbind(result_df, data.frame(
  Province = "Liaoning",
  TE_Temp_to_Ozone = 0.01130649,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.004852319,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Zhejiang",
  TE_Temp_to_Ozone = 0.02086674,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.005229586,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Beijing",
  TE_Temp_to_Ozone = 0.01031306,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.008503927,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Guangxi",
  TE_Temp_to_Ozone = 0.009747755,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.006231624,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Jiangsu",
  TE_Temp_to_Ozone = 0.01470588,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.006120615,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Jiangxi",
  TE_Temp_to_Ozone = 0.01011489,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.007545881,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Fujian",
  TE_Temp_to_Ozone = 0.01119572,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.00540621,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Anhui",
  TE_Temp_to_Ozone = 0.01657674,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.01153604,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Shanxi",
  TE_Temp_to_Ozone = 0.04545306,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.01002086,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Tianjin",
  TE_Temp_to_Ozone = 0.0182848,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.006434189,
  P_Ozone_to_Temp = "0.02 *",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Henan",
  TE_Temp_to_Ozone = 0.01471364,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.01346229,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Sichuan",
  TE_Temp_to_Ozone = 0.01888677,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.01540293,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Hubei",
  TE_Temp_to_Ozone = 0.01241405,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.005219146,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Guangdong",
  TE_Temp_to_Ozone = 0.01106916,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.004739848,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Shandong",
  TE_Temp_to_Ozone = 0.02083901,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.00934926,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Jilin",
  TE_Temp_to_Ozone = 0.01520015,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.007630436,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

result_df <- rbind(result_df, data.frame(
  Province = "Heilongjiang",
  TE_Temp_to_Ozone = 0.006557175,
  P_Temp_to_Ozone = "0.00 ***",
  TE_Ozone_to_Temp = 0.003657998,
  P_Ozone_to_Temp = "0.00 ***",
  Dominant_Direction = "Temperature precedes Ozone"
))

print(result_df)



# TE bar chart
library(readxl)
library(ggplot2)
library(tidyr)
library(dplyr)
df <- read_excel("TE.xlsx")

colnames(df) <- c("Province", "Heat_to_Ozone", "Ozone_to_Heat", "Difference")

df$Province <- factor(df$Province, levels = c(df$Province[df$Province != "Nation"], "Nation"))

# transform to long format
df_long <- df %>%
  pivot_longer(cols = c("Heat_to_Ozone", "Ozone_to_Heat", "Difference"),
               names_to = "Type",
               values_to = "Value")

# define color
color_mapping <- c("Heat_to_Ozone" = "#234086", 
                   "Ozone_to_Heat" = "#659bca", 
                   "Difference" = "#bbcfe8")

# TE bar chart
p <- ggplot(df_long, aes(x = Value, y = Province, fill = Type)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.8) +
  scale_fill_manual(values = color_mapping,
                    labels = c("Difference (Heat→Ozone - Ozone→Heat)", "Heat→Ozone TE", "Ozone→Heat TE")) +
  scale_x_continuous(limits = c(0, 0.05), expand = c(0, 0),
                     breaks = seq(0, 0.05, by = 0.01)) +
  labs(x = "Transfer Entropy Values", 
       y = "Province",
       title = "Transfer Entropy of Extreme Heat and Surface Ozone Pollution") +
  theme_minimal(base_size = 14) +
  theme(
    axis.title = element_text(size = 18, face = "bold"),
    axis.text.y = element_text(size = 16, color = "black", hjust = 1),
    axis.text.x = element_text(size = 16, color = "black"),
    legend.position = c(0.85, 0.90),  
    legend.title = element_blank(),
    legend.text = element_text(size = 16),
    legend.background = element_rect(fill = "white", color = "grey", linewidth = 0.5),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    plot.margin = margin(1, 1, 1, 1, "cm")
  )

print(p)
ggsave("Transfer_Entropy_Plot.png", p, 
       width = 16, height = 14, dpi = 400, 
       units = "in", bg = "white")


